---
layout: post
title: "Fine-tuning human for LLM projects"
excerpt: "In this post, I will talk about a method which I have designed to fine tune a human being to lower the expectation from LLM models."
tags: [nlp, psychology]
categories: [NLP, LLM, Psychology]
share: true
comments: true
mathjax: false
---

In this post, I will talk about a method which I have designed to fine tune a human being to lower the expectation from LLM models.

This technique more of a psychological method than a technological way to improve the models output to be more human like.

I have designed a multiple choice questionnaire to adapt a human expectation, but it will be a type of cognition control strategy.

Cognitive control is a broad class of mental operations that include goal representation, attention allocation, and stimulus-response mapping. Cognitive control is important when there is competition for limited mental resources. It helps reduce uncertainty in decision-making by controlling what information reaches focused awareness. According to the Expected Value of Control theory, people integrate information about the expected reward and efficacy of task performance to determine the expected value of control. They then adjust their control allocation (i.e., mental effort) accordingly.

The way I designed the MCQs here, it will slowly indicate the human user (who is interacting with test) about the right expectation from the model using multiple scenarios and positive feedback.

The concept which I thought worked by adjusting the expectation of the end user by promoting what can you expect from the LLM models. Testing LLM models are quite challenging as there is no standard framework around it. This actually worked for two use cases where I have implemented this. It is very complicated to verify and evaluate the efficiency of the method proposed method whether it will work properly or not but after practically implementing this I can conform it worked seamlessly for 2 use cases.

* Template:
  * LLM for Scenario X
    - Q(1)
      * Human Answer1
      * Human Answer2
      * LLM output
      * Human Answer3
    * Q(2)
      * Human Answer1
      * Human Answer2
      * LLM output
      * Human Answer3
    * Q(n/2)
      * Human Answer1
      * LLM output1 
      * LLM output2
      * Human Answer2
    * Q(n)
      * Human Answer1
      * LLM output1 
      * LLM output2
      * LLM output3

As the questions increases then no. of LLM outputs should increase which increase the probability of selection of LLM output choices this will boost the confidence and morale of the end user that they are able to guess the right source of the data.

If the user selects Human answer then we will be showing the end-user a positive message saying "You are close enough but, the machine will provide you with the following answer ...". If the end-user selects the LLM answer then we will again prompt with a positive message saying "Machine is thinking like you, and you are right! This is the answer which machine will generate." 

We had around 10 questions for multiple set of documents more than 4, and we tested the end users reaction before and after taking the test. The entire outlook about LLM models changed after going through the designed tests. Along with this it was informed that this is more of a preview and LLM will work as an aid and with continuous expert feedback we will be able to reach the desired accuracy with iterative improvements.

Some visible changes which I have observed are:
- Expectation got lowered
- Gave more constructive feedback (before the test the users were not that ready to accept the solution)
- Still had reservations but open to accept the future of the product


### Reference
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285389/#:~:text=Cognitive%20control%20of%20behavior%20is,et%20al.%2C%201996).
- https://www.sciencedirect.com/science/article/pii/S1364661321001480#:~:text=Cognitive%20control%20can%20be%20defined,how%20cognitive%20control%20is%20implemented.
- https://pubmed.ncbi.nlm.nih.gov/33589626/