---
layout: post
title: "[Paper] Fine-tuning human for LLM projects"
excerpt: "In this post, I will talk about a method which I have designed to fine tune a human being to lower the expectation from LLM models."
tags: [nlp, psychology]
categories: [NLP, LLM, Psychology]
share: true
link: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4574477
comments: true
mathjax: false
---

In recent years after emerging of LLM in the space of NLP, tools like ChatGPT, BARD, DALL-E, took over the market and daily lives. We will be focusing on the human like output capability from LLMâ€™s. There is a huge reservation of this technology due to multiple scenarios like security, accuracy, relevance, etc... In this paper, I will talk about a method which I have designed to fine tune a human being to lower the expectation from LLM outputs and increase the acceptance rate of the final product. This technique is more of a psychological method than a technological way to improve the models output to be more human like.

## Paper Link

### [SSRN (link)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4574477)
<iframe width="100%" height="400" src="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4574477" allowfullscreen="allowfullscreen"></iframe>

### [OSF (link)](https://osf.io/9js3b)
<iframe width="100%" height="400" src="https://osf.io/9js3b" allowfullscreen="allowfullscreen"></iframe>
